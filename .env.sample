# Zmień poniższą wartość na identyfikator Twoich warsztatów -
# zgodny z identyfikatorem OnRamp Cloud Credits: trygcp.dev/claim/<IDENTYFIKATOR>
BIELIK_EVENT_ID="bielik-dw-01"

# GPU w Cloud Run są dostępne tylko w wybranych lokalizacjach:
# https://cloud.google.com/run/docs/configuring/services/gpu
GOOGLE_CLOUD_LOCATION="europe-west1"  # Europa (Belgia)

# Nazwa usługi Cloud Run w której hostujesz model Bielik w oparciu o Ollama
BIELIK_SERVICE_NAME="ollama-bielik-v3"

# Nazwa modelu Bielik używanego w aplikacji
# Jeżeli chcesz zmienić domyślny model Bielik na inną wersję,
# pamiętaj, aby zainstalować taką samą wersję w Ollama.
# W powyższym przypadku, zmień również linię: "ENV MODEL" w pliku ollama-bielik/Dockerfile
# i ponownie zainstaluj kontener ollama-bielik w Cloud Run.
BIELIK_MODEL_NAME="SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0"
#SpeakLeash/bielik-11b-v2.3-instruct:Q8_0
#SpeakLeash/bielik-11b-v2.3-instruct:Q6_K
#SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M

# Przypisz do zmiennej OLLAMA_API_BASE URL usługi Cloud Run hostującej Ollama z modelem Bielik.
# URL znajdziesz w konsoli Google cloud.
# Możesz również uruchomić poniższe polecenie w Cloud Shell:
# gcloud run services describe $BIELIK_SERVICE_NAME --region=$GOOGLE_CLOUD_LOCATION --format='value(status.url)'
# Następnie odkomentuj poniższą linię i wklej tam URL usługi Cloud Run.
# OLLAMA_API_BASE=

# Jeżeli chcesz używać Gemini przez usługę Vertex AI, ustaw poniższą wartość na TRUE
GOOGLE_GENAI_USE_VERTEXAI=FALSE

# Wklej tutaj klucz do GEMINI API - możesz go stworzyć w AI Studio: https://ai.dev
#GOOGLE_API_KEY=